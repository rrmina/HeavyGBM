{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # For type hinting my own class!\n",
    "\n",
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self,\n",
    "        min_samples_per_node: int = 5,\n",
    "        max_depth: int = 5,\n",
    "        impurity_measure: str = 'gini',\n",
    "        num_targets: int = None\n",
    "    ) -> None:\n",
    "\n",
    "        # Decision Tree Hyperparameters\n",
    "        self.min_samples_per_node = min_samples_per_node\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Impurity Functions\n",
    "        self.impurity_measure = impurity_measure\n",
    "        impurity_functions = {\n",
    "            'gini': self._compute_gini_impurity,\n",
    "            'entropy': self._compute_entropy,\n",
    "            'variance': self._compute_variance\n",
    "        }\n",
    "        self.impurity_function = impurity_functions[impurity_measure]\n",
    "\n",
    "        # And Prediction Storage Functions\n",
    "        pred_storage_functions = {\n",
    "            'gini': self._store_classification_pred,\n",
    "            'entropy': self._store_classification_pred,\n",
    "            'variance': self._store_regression_pred\n",
    "        }\n",
    "        self.pred_storage_function = pred_storage_functions[impurity_measure]\n",
    "\n",
    "        # Number of targets for counting purpose\n",
    "        self.num_targets =  num_targets # to be determined\n",
    "\n",
    "        # Class Variables\n",
    "        self.depth = 0\n",
    "\n",
    "        # Node Variables\n",
    "        self.left_node = None\n",
    "        self.right_node = None\n",
    "        self.splitting_feature = None\n",
    "        self.splitting_threshold = None\n",
    "        self.pred = None\n",
    "\n",
    "    ######################################################################################################\n",
    "    #\n",
    "    #                                         Training Methods\n",
    "    #\n",
    "    ######################################################################################################\n",
    "\n",
    "\n",
    "    # Currently writing for training only\n",
    "    # I will ammend this to enable prediction\n",
    "    def build_tree(self, \n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray,\n",
    "        depth: int = 0\n",
    "    ) -> DecisionTree:\n",
    "        \n",
    "        # Store depth\n",
    "        self.depth = depth\n",
    "\n",
    "        # Check stopping criterion\n",
    "        if self.check_stopping_criterion(X, y) is True:\n",
    "            self.pred = self.pred_storage_function(y)\n",
    "            return self\n",
    "\n",
    "        # Get the Decision Split and the Data Split\n",
    "        data_splits, feature_index, threshold = self.find_best_split(X, y)\n",
    "        if data_splits is None:\n",
    "            return None\n",
    "        self.splitting_feature = feature_index\n",
    "        self.splitting_threshold = threshold\n",
    "\n",
    "        # Store the prediction\n",
    "        self.pred = self.pred_storage_function(y)\n",
    "\n",
    "        # Build Left and Right (Children) Tree\n",
    "        (X_left, y_left), (X_right, y_right) = data_splits \n",
    "        self.left_node = DecisionTree(\n",
    "            min_samples_per_node = self.min_samples_per_node,\n",
    "            max_depth = self.max_depth,\n",
    "            impurity_measure = self.impurity_measure,\n",
    "            num_targets = self.num_targets\n",
    "        ).build_tree(X_left, y_left, depth + 1)     # Add 1 to depth\n",
    "\n",
    "        self.right_node = DecisionTree(\n",
    "            min_samples_per_node = self.min_samples_per_node,\n",
    "            max_depth = self.max_depth,\n",
    "            impurity_measure = self.impurity_measure,\n",
    "            num_targets = self.num_targets\n",
    "        ).build_tree(X_right, y_right, depth + 1)   # Add 1 to depth\n",
    "\n",
    "        # Return the Node\n",
    "        return self\n",
    "\n",
    "    def check_stopping_criterion(self,\n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> bool:\n",
    "        \n",
    "        # Check max depth\n",
    "        if self.depth > self.max_depth:\n",
    "            print(\"Stopping with max_depth: \", self.max_depth)\n",
    "            return True\n",
    "        \n",
    "        # Check minimum number of samples per node\n",
    "        if X.shape[0] <= self.min_samples_per_node:\n",
    "            print(\"Stopping with min_samples_per_node: \", X.shape[0])\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def find_best_split(self,\n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> Tuple[\n",
    "        Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]],    # Left Data and Right Data\n",
    "        Union[int, str],                                                        # Feature index or feature name \n",
    "        float                                                                   # Threshold\n",
    "    ]:\n",
    "        \n",
    "        # Find the best split\n",
    "        best_split = None\n",
    "        best_impurity = np.inf\n",
    "        best_threshold = None\n",
    "        best_feature = None\n",
    "\n",
    "        num_samples, num_feautures = X.shape[0], X.shape[1]\n",
    "        for i in range(num_feautures):\n",
    "\n",
    "            # TO BE IMPROVED by using buckets / histograms\n",
    "            # Find the threshold with the least impurity\n",
    "            thresholds = np.unique(X[:, i])      \n",
    "            for threshold in thresholds:\n",
    "                left_data, right_data = self.split_data(X, i, y, threshold)\n",
    "                X_left, y_left = left_data\n",
    "                X_right, y_right = right_data\n",
    "\n",
    "                # Skip the lowest and highest values because no split\n",
    "                if (X_left.shape[0] == 0) or (X_right.shape[0] == 0):\n",
    "                    continue\n",
    "\n",
    "                # Compute impurity\n",
    "                left_impurity = self.impurity_function(y_left)\n",
    "                right_impurity = self.impurity_function(y_right)\n",
    "                impurity = (y_left.shape[0] / y.shape[0]) * left_impurity + \\\n",
    "                    (y_right.shape[0] / y.shape[0]) * right_impurity\n",
    "                \n",
    "                # Compare with the best impurity\n",
    "                if impurity < best_impurity:\n",
    "                    best_split = (left_data, right_data)\n",
    "                    best_impurity = impurity\n",
    "                    best_feature = i\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # If no best split\n",
    "        # Return (data split, feature/feature name, feature threshold)\n",
    "        if best_split is None:\n",
    "            return None, None, None\n",
    "\n",
    "        # If same number of samples after best split, return None\n",
    "        # TO BE IMPROVED\n",
    "        # Do we need this?\n",
    "        num_samples_left = best_split[0][0].shape[0]\n",
    "        num_samples_right = best_split[1][0].shape[0]\n",
    "        if (num_samples_left == 0) or (num_samples_right == 0):\n",
    "            print(\"im here\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Normally we would compare the parent's impurities to the \n",
    "        # children's impurities. But mathematically, the children's impurities \n",
    "        # will always be less or equal to the parent's impurities\n",
    "\n",
    "        return best_split, best_feature, best_threshold\n",
    "    \n",
    "    def split_data(self,\n",
    "        X: np.ndarray,\n",
    "        i: Union[int, str],\n",
    "        y: np.ndarray,\n",
    "        threshold: float\n",
    "    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]: \n",
    "        \n",
    "        # Split mask according to a specific feature\n",
    "        X_i = X[:, i]\n",
    "        left_indices = np.where(X_i <= threshold)\n",
    "        right_indices = np.where(X_i > threshold)\n",
    "        \n",
    "        # Apply split mask to the whole dataset\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        return (X_left, y_left), (X_right, y_right)\n",
    "\n",
    "    ######################################################################################################\n",
    "    #\n",
    "    #                               Prediction and Prediction Storage Methods\n",
    "    #\n",
    "    ######################################################################################################    \n",
    "\n",
    "    def predict(self, \n",
    "        X: np.ndarray\n",
    "    ) -> Union[int, float]:\n",
    "        \n",
    "        # If node is a leaf\n",
    "        if self.left_node is None and self.right_node is None:\n",
    "            return self.pred\n",
    "        \n",
    "        # Else traverse the tree by recursion\n",
    "        is_left = X[self.splitting_feature] <= self.splitting_threshold\n",
    "        if is_left:\n",
    "            return self.left_node.predict(X)\n",
    "        else:\n",
    "            return self.right_node.predict(X)\n",
    "\n",
    "    def _store_classification_pred(self, \n",
    "        y: np.ndarray   # int\n",
    "    ) -> int:\n",
    "        target_dist = self._compute_target_dist(y)\n",
    "        best_class = np.argmax(target_dist)\n",
    "\n",
    "        return best_class\n",
    "\n",
    "    def _store_regression_pred(self, \n",
    "        y: np.ndarray   # float\n",
    "    ) -> float:\n",
    "        best_pred = np.mean(y)\n",
    "        \n",
    "        return best_pred\n",
    "\n",
    "    ######################################################################################################\n",
    "    #\n",
    "    #                                     Visualization Methods\n",
    "    #\n",
    "    ######################################################################################################    \n",
    "\n",
    "    def visualize_decision_nodes(self, \n",
    "        depth: int = 0\n",
    "    ) -> None:\n",
    "\n",
    "        # Indentation to represent tree depth\n",
    "        indent = \" \" * (4 * depth)\n",
    "\n",
    "        # If the node is a leaf, print the prediction\n",
    "        if self.left_node is None and self.right_node is None:\n",
    "            print(f\"{indent}Leaf Node: Prediction = {self.pred}\")\n",
    "            return\n",
    "\n",
    "        # Print the current node's feature and threshold\n",
    "        print(f\"{indent}Node: Feature[{self.splitting_feature}] <= {self.splitting_threshold}\")\n",
    "\n",
    "        # Recur for left and right children\n",
    "        if self.left_node is not None:\n",
    "            print(f\"{indent}  Left:\")\n",
    "            self.left_node.visualize_decision_nodes(depth + 1)\n",
    "        \n",
    "        if self.right_node is not None:\n",
    "            print(f\"{indent}  Right:\")\n",
    "            self.right_node.visualize_decision_nodes(depth + 1)\n",
    "\n",
    "\n",
    "    ######################################################################################################\n",
    "    #\n",
    "    #                        Impurity Methods | Supposedly methods of a Node Class\n",
    "    #\n",
    "    ######################################################################################################\n",
    "    def _compute_target_dist(self, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \n",
    "        # Count the number of samples per target\n",
    "        # TO BE IMPROVED\n",
    "        target_dist = []\n",
    "        for i in range(self.num_targets):\n",
    "            target_dist.append(np.mean((y == i)*1))\n",
    "\n",
    "        return target_dist\n",
    "\n",
    "    # For Classification\n",
    "    def _compute_gini_impurity(self, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \n",
    "        target_dist = np.array(self._compute_target_dist(y))\n",
    "        gini_impurity = np.sum(target_dist * (1-target_dist))\n",
    "\n",
    "        return gini_impurity\n",
    "    \n",
    "    # For Classification\n",
    "    def _compute_entropy(self,\n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \n",
    "        target_dist = np.array(self._compute_target_dist(y))\n",
    "        target_dist = target_dist[target_dist > 0]                  # Remove zero probabilities\n",
    "        entropy = -1 * np.sum(target_dist * np.log2(target_dist))   # Zero prob results to log undef\n",
    "\n",
    "        return entropy\n",
    "    \n",
    "    # For Regression\n",
    "    def _compute_variance(self,\n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \n",
    "        num_samples = y.shape[0]\n",
    "        if num_samples == 0:\n",
    "            return 0\n",
    "        \n",
    "        insides = y - np.mean(y)\n",
    "        squares = np.power(insides, 2)\n",
    "        variance = np.mean(squares)\n",
    "\n",
    "        return variance\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset | Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target labels: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"Target labels:\", target_names)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  4\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  4\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  4\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTree(\n",
    "    min_samples_per_node=5,\n",
    "    max_depth=np.inf,\n",
    "    impurity_measure='entropy',\n",
    "    num_targets=3\n",
    ")\n",
    "\n",
    "wawa = decision_tree.build_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "0 0\n",
      "2 2\n",
      "0 0\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "for inde in range(X_test.shape[0]):\n",
    "    print(wawa.predict(X_test[inde]), y_test[inde])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Dataset | Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "X, y = cancer.data.to_numpy(), cancer.target.to_numpy()\n",
    "# print(X.head())  # Display first few rows\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(cancer.target_names)  # Display target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree(\n",
    "    min_samples_per_node=5,\n",
    "    max_depth=10,\n",
    "    impurity_measure='entropy',\n",
    "    num_targets=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  4\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  3\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  5\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n"
     ]
    }
   ],
   "source": [
    "wawa = decision_tree.build_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "0 1\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "for inde in range(X_test.shape[0]):\n",
    "    print(wawa.predict(X_test[inde]), y_test[inde])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Dataset | Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes(as_frame=True)\n",
    "X, y = diabetes.data.to_numpy(), diabetes.target.to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTree(\n",
    "    min_samples_per_node=2,\n",
    "    max_depth=10,\n",
    "    impurity_measure='variance',\n",
    "    # num_targets=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with max_depth:  10\n",
      "Stopping with max_depth:  10\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n",
      "Stopping with min_samples_per_node:  1\n",
      "Stopping with min_samples_per_node:  2\n"
     ]
    }
   ],
   "source": [
    "wawa = decision_tree.build_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.4 219.0\n",
      "166.0 70.0\n",
      "187.4 202.0\n",
      "150.0 230.0\n",
      "230.0 111.0\n",
      "87.0 84.0\n",
      "235.0 242.0\n",
      "248.0 272.0\n",
      "203.0 94.0\n",
      "157.71428571428572 96.0\n",
      "113.0 94.0\n",
      "200.0 252.0\n",
      "71.33333333333333 99.0\n",
      "214.0 297.0\n",
      "78.80952380952381 135.0\n",
      "60.0 67.0\n",
      "296.0 295.0\n",
      "190.0 264.0\n",
      "293.0 170.0\n",
      "178.0 275.0\n",
      "165.5 310.0\n",
      "62.0 64.0\n",
      "48.4 128.0\n",
      "220.57142857142858 232.0\n",
      "87.5 129.0\n",
      "157.71428571428572 118.0\n",
      "282.0 263.0\n",
      "111.5 77.0\n",
      "59.0 48.0\n",
      "230.0 107.0\n",
      "292.5 140.0\n",
      "55.0 113.0\n",
      "224.0 90.0\n",
      "287.5 164.0\n",
      "165.5 180.0\n",
      "190.0 233.0\n",
      "78.5 42.0\n",
      "125.5 84.0\n",
      "206.75 172.0\n",
      "96.0 63.0\n",
      "59.0 48.0\n",
      "66.0 108.0\n",
      "209.0 156.0\n",
      "169.5 168.0\n",
      "187.4 90.0\n",
      "73.5 52.0\n",
      "31.0 200.0\n",
      "174.5 87.0\n",
      "59.0 90.0\n",
      "179.0 258.0\n",
      "141.5 136.0\n",
      "73.5 158.0\n",
      "144.5 69.0\n",
      "174.5 72.0\n",
      "292.0 171.0\n",
      "141.5 95.0\n",
      "42.0 72.0\n",
      "273.5 151.0\n",
      "42.0 168.0\n",
      "58.0 60.0\n",
      "67.0 122.0\n",
      "165.5 52.0\n",
      "141.5 187.0\n",
      "88.0 102.0\n",
      "124.0 214.0\n",
      "236.0 248.0\n",
      "92.0 181.0\n",
      "101.5 110.0\n",
      "150.0 140.0\n",
      "179.0 202.0\n",
      "179.0 101.0\n",
      "244.0 222.0\n",
      "243.0 281.0\n",
      "111.5 61.0\n",
      "73.5 89.0\n",
      "238.5 91.0\n",
      "182.5 186.0\n",
      "188.0 220.0\n",
      "157.71428571428572 237.0\n",
      "238.5 233.0\n",
      "48.4 68.0\n",
      "78.80952380952381 190.0\n",
      "58.0 96.0\n",
      "42.0 72.0\n",
      "49.0 153.0\n",
      "52.0 98.0\n",
      "74.0 37.0\n",
      "74.0 63.0\n",
      "147.0 184.0\n"
     ]
    }
   ],
   "source": [
    "for inde in range(X_test.shape[0]):\n",
    "    print(wawa.predict(X_test[inde]), y_test[inde])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
