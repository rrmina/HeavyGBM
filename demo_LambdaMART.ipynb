{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import annotations # For type hinting my own class!\n",
    "from DecisionTreeRegressor import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: X shape (9219, 25), y shape (9219,)\n",
      "Validation set: X shape (3538, 25), y shape (3538,)\n",
      "Test set: X shape (3383, 25), y shape (3383,)\n",
      "First 5 training samples:\n",
      " [[ 3.00000000e+00  2.07944154e+00  2.72727270e-01  2.61034130e-01\n",
      "   3.73305651e+01  1.14312412e+01  3.72997501e+01  1.13865735e+00\n",
      "   1.55242894e+01  8.83129655e+00  1.20000000e+01  5.37527841e+00\n",
      "   8.75912400e-02  8.64936400e-02  2.83030646e+01  9.34002375e+00\n",
      "   2.48087847e+01  3.93090680e-01  5.74165170e+01  3.29489291e+00\n",
      "   2.50231000e+01  3.21979940e+00 -3.87098000e+00 -3.90273000e+00\n",
      "  -3.87512000e+00]\n",
      " [ 3.00000000e+00  2.07944154e+00  4.28571430e-01  4.00594180e-01\n",
      "   3.73305651e+01  1.14312412e+01  3.72997501e+01  1.81447983e+00\n",
      "   1.74549923e+01  1.16179306e+01  1.00000000e+01  5.19295685e+00\n",
      "   8.54700900e-02  8.45371100e-02  2.83030646e+01  9.34002375e+00\n",
      "   2.48087847e+01  3.49204570e-01  4.32406261e+01  2.65472417e+00\n",
      "   2.34903000e+01  3.15658757e+00 -3.96838000e+00 -4.00865000e+00\n",
      "  -3.98670000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.73305651e+01  1.14312412e+01  3.72997501e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  8.00000000e+00  4.38202663e+00\n",
      "   7.69230800e-02  7.60181300e-02  2.83030646e+01  9.34002375e+00\n",
      "   2.48087847e+01  2.40318870e-01  2.58169894e+01  1.55134225e+00\n",
      "   1.58650000e+01  2.76411543e+00 -4.28166000e+00 -4.33313000e+00\n",
      "  -4.44161000e+00]\n",
      " [ 4.00000000e+00  2.77258872e+00  3.33333330e-01  3.20170830e-01\n",
      "   3.73305651e+01  1.14312412e+01  3.72997501e+01  1.26080803e+00\n",
      "   1.79752418e+01  8.86378153e+00  3.00000000e+00  1.79175947e+00\n",
      "   3.40909100e-02  3.37724100e-02  2.83030646e+01  9.34002375e+00\n",
      "   2.48087847e+01  1.11496400e-01  1.00924259e+01  6.49758360e-01\n",
      "   1.42778000e+01  2.65870588e+00 -4.77772000e+00 -4.73563000e+00\n",
      "  -4.86759000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   3.73305651e+01  1.14312412e+01  3.72997501e+01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  6.00000000e+00  3.87120101e+00\n",
      "   4.76190500e-02  4.73690700e-02  2.83030646e+01  9.34002375e+00\n",
      "   2.48087847e+01  1.82104030e-01  2.35462963e+01  1.62139253e+00\n",
      "   1.52764000e+01  2.72630915e+00 -4.43073000e+00 -4.45985000e+00\n",
      "  -4.57053000e+00]]\n",
      "First 5 relevance labels:\n",
      " [2 0 2 2 0]\n",
      "First 5 query IDs:\n",
      " [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_letor_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a LETOR-formatted file into features and relevance labels.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the LETOR file.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Feature matrix of shape (num_samples, num_features).\n",
    "        y (np.ndarray): Relevance labels of shape (num_samples, ).\n",
    "        qids (list): Query IDs corresponding to each sample.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    query_ids = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Remove comments and split the line\n",
    "            line = line.split('#')[0].strip()\n",
    "            parts = line.split()\n",
    "\n",
    "            # Extract relevance score, query ID, and features\n",
    "            relevance = int(parts[0])\n",
    "            qid = int(parts[1].split(':')[1])\n",
    "            feature_values = [float(x.split(':')[1]) for x in parts[2:]]\n",
    "\n",
    "            # Append to respective lists\n",
    "            labels.append(relevance)\n",
    "            query_ids.append(qid)\n",
    "            features.append(feature_values)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    return X, y, query_ids\n",
    "\n",
    "# Paths to the dataset files\n",
    "train_file = 'OHSUMED/Data/Fold1/trainingset.txt'  # Replace with actual path\n",
    "test_file = 'OHSUMED/Data/Fold1/testset.txt'\n",
    "valid_file = 'OHSUMED/Data/Fold1/validationset.txt'\n",
    "\n",
    "# Load datasets\n",
    "X_train, y_train, qids_train = load_letor_file(train_file)\n",
    "X_valid, y_valid, qids_valid = load_letor_file(valid_file)\n",
    "X_test, y_test, qids_test = load_letor_file(test_file)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Train set: X shape {X_train.shape}, y shape {y_train.shape}\")\n",
    "print(f\"Validation set: X shape {X_valid.shape}, y shape {y_valid.shape}\")\n",
    "print(f\"Test set: X shape {X_test.shape}, y shape {y_test.shape}\")\n",
    "\n",
    "# Example output (first 5 rows)\n",
    "print(\"First 5 training samples:\\n\", X_train[:5])\n",
    "print(\"First 5 relevance labels:\\n\", y_train[:5])\n",
    "print(\"First 5 query IDs:\\n\", qids_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM LambdaMART model...\n",
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4077\n",
      "[LightGBM] [Info] Number of data points in the train set: 9219, number of used features: 25\n",
      "[LightGBM] [Warning] Unknown parameter: max_position\n",
      "[1]\ttrain's ndcg@1: 0.444444\ttrain's ndcg@2: 0.430117\ttrain's ndcg@3: 0.452901\ttrain's ndcg@4: 0.451371\ttrain's ndcg@5: 0.452569\tvalid's ndcg@1: 0.190476\tvalid's ndcg@2: 0.294865\tvalid's ndcg@3: 0.318789\tvalid's ndcg@4: 0.321234\tvalid's ndcg@5: 0.306383\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\ttrain's ndcg@1: 0.772487\ttrain's ndcg@2: 0.6856\ttrain's ndcg@3: 0.639455\ttrain's ndcg@4: 0.600256\ttrain's ndcg@5: 0.581698\tvalid's ndcg@1: 0.222222\tvalid's ndcg@2: 0.320471\tvalid's ndcg@3: 0.345835\tvalid's ndcg@4: 0.354408\tvalid's ndcg@5: 0.356392\n",
      "[3]\ttrain's ndcg@1: 0.777778\ttrain's ndcg@2: 0.702461\ttrain's ndcg@3: 0.665371\ttrain's ndcg@4: 0.637833\ttrain's ndcg@5: 0.63357\tvalid's ndcg@1: 0.412698\tvalid's ndcg@2: 0.449542\tvalid's ndcg@3: 0.448346\tvalid's ndcg@4: 0.442352\tvalid's ndcg@5: 0.403899\n",
      "[4]\ttrain's ndcg@1: 0.825397\ttrain's ndcg@2: 0.737799\ttrain's ndcg@3: 0.707315\ttrain's ndcg@4: 0.684402\ttrain's ndcg@5: 0.660296\tvalid's ndcg@1: 0.428571\tvalid's ndcg@2: 0.428571\tvalid's ndcg@3: 0.43602\tvalid's ndcg@4: 0.434768\tvalid's ndcg@5: 0.412087\n",
      "[5]\ttrain's ndcg@1: 0.84127\ttrain's ndcg@2: 0.751625\ttrain's ndcg@3: 0.726587\ttrain's ndcg@4: 0.698334\ttrain's ndcg@5: 0.676558\tvalid's ndcg@1: 0.428571\tvalid's ndcg@2: 0.379447\tvalid's ndcg@3: 0.398423\tvalid's ndcg@4: 0.419504\tvalid's ndcg@5: 0.413404\n",
      "[6]\ttrain's ndcg@1: 0.851852\ttrain's ndcg@2: 0.788816\ttrain's ndcg@3: 0.748844\ttrain's ndcg@4: 0.71507\ttrain's ndcg@5: 0.689289\tvalid's ndcg@1: 0.428571\tvalid's ndcg@2: 0.385588\tvalid's ndcg@3: 0.388225\tvalid's ndcg@4: 0.405683\tvalid's ndcg@5: 0.405362\n",
      "[7]\ttrain's ndcg@1: 0.846561\ttrain's ndcg@2: 0.791712\ttrain's ndcg@3: 0.747337\ttrain's ndcg@4: 0.708366\ttrain's ndcg@5: 0.688677\tvalid's ndcg@1: 0.444444\tvalid's ndcg@2: 0.413742\tvalid's ndcg@3: 0.398599\tvalid's ndcg@4: 0.416982\tvalid's ndcg@5: 0.409153\n",
      "[8]\ttrain's ndcg@1: 0.846561\ttrain's ndcg@2: 0.797853\ttrain's ndcg@3: 0.757002\ttrain's ndcg@4: 0.712961\ttrain's ndcg@5: 0.690573\tvalid's ndcg@1: 0.412698\tvalid's ndcg@2: 0.412698\tvalid's ndcg@3: 0.420147\tvalid's ndcg@4: 0.421564\tvalid's ndcg@5: 0.406922\n",
      "[9]\ttrain's ndcg@1: 0.835979\ttrain's ndcg@2: 0.791364\ttrain's ndcg@3: 0.758904\ttrain's ndcg@4: 0.723826\ttrain's ndcg@5: 0.70479\tvalid's ndcg@1: 0.492063\tvalid's ndcg@2: 0.461361\tvalid's ndcg@3: 0.438769\tvalid's ndcg@4: 0.434386\tvalid's ndcg@5: 0.426392\n",
      "[10]\ttrain's ndcg@1: 0.830688\ttrain's ndcg@2: 0.792925\ttrain's ndcg@3: 0.745295\ttrain's ndcg@4: 0.728288\ttrain's ndcg@5: 0.707342\tvalid's ndcg@1: 0.492063\tvalid's ndcg@2: 0.442939\tvalid's ndcg@3: 0.443292\tvalid's ndcg@4: 0.443486\tvalid's ndcg@5: 0.432216\n",
      "[11]\ttrain's ndcg@1: 0.857143\ttrain's ndcg@2: 0.804341\ttrain's ndcg@3: 0.759365\ttrain's ndcg@4: 0.740846\ttrain's ndcg@5: 0.720442\tvalid's ndcg@1: 0.52381\tvalid's ndcg@2: 0.437842\tvalid's ndcg@3: 0.465462\tvalid's ndcg@4: 0.451254\tvalid's ndcg@5: 0.44313\n",
      "[12]\ttrain's ndcg@1: 0.846561\ttrain's ndcg@2: 0.806751\ttrain's ndcg@3: 0.753394\ttrain's ndcg@4: 0.742143\ttrain's ndcg@5: 0.717851\tvalid's ndcg@1: 0.539683\tvalid's ndcg@2: 0.447575\tvalid's ndcg@3: 0.491533\tvalid's ndcg@4: 0.464936\tvalid's ndcg@5: 0.448768\n",
      "[13]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.827066\ttrain's ndcg@3: 0.781357\ttrain's ndcg@4: 0.751292\ttrain's ndcg@5: 0.725158\tvalid's ndcg@1: 0.539683\tvalid's ndcg@2: 0.459856\tvalid's ndcg@3: 0.474862\tvalid's ndcg@4: 0.469748\tvalid's ndcg@5: 0.475858\n",
      "[14]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.825019\ttrain's ndcg@3: 0.77979\ttrain's ndcg@4: 0.756216\ttrain's ndcg@5: 0.732327\tvalid's ndcg@1: 0.587302\tvalid's ndcg@2: 0.507475\tvalid's ndcg@3: 0.49641\tvalid's ndcg@4: 0.487673\tvalid's ndcg@5: 0.48723\n",
      "[15]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.833207\ttrain's ndcg@3: 0.783574\ttrain's ndcg@4: 0.762904\ttrain's ndcg@5: 0.733982\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.522304\tvalid's ndcg@3: 0.511484\tvalid's ndcg@4: 0.502882\tvalid's ndcg@5: 0.494196\n",
      "[16]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.841394\ttrain's ndcg@3: 0.797409\ttrain's ndcg@4: 0.768277\ttrain's ndcg@5: 0.738711\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.545361\tvalid's ndcg@3: 0.510509\tvalid's ndcg@4: 0.504739\tvalid's ndcg@5: 0.491644\n",
      "[17]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.851628\ttrain's ndcg@3: 0.813382\ttrain's ndcg@4: 0.786464\ttrain's ndcg@5: 0.751627\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.545361\tvalid's ndcg@3: 0.529131\tvalid's ndcg@4: 0.52023\tvalid's ndcg@5: 0.496772\n",
      "[18]\ttrain's ndcg@1: 0.873016\ttrain's ndcg@2: 0.853675\ttrain's ndcg@3: 0.80768\ttrain's ndcg@4: 0.785388\ttrain's ndcg@5: 0.75652\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.563783\tvalid's ndcg@3: 0.54323\tvalid's ndcg@4: 0.521284\tvalid's ndcg@5: 0.501853\n",
      "[19]\ttrain's ndcg@1: 0.883598\ttrain's ndcg@2: 0.857405\ttrain's ndcg@3: 0.805936\ttrain's ndcg@4: 0.785661\ttrain's ndcg@5: 0.763684\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.551502\tvalid's ndcg@3: 0.537555\tvalid's ndcg@4: 0.508557\tvalid's ndcg@5: 0.494739\n",
      "[20]\ttrain's ndcg@1: 0.883598\ttrain's ndcg@2: 0.857405\ttrain's ndcg@3: 0.80966\ttrain's ndcg@4: 0.791446\ttrain's ndcg@5: 0.767734\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.551502\tvalid's ndcg@3: 0.533831\tvalid's ndcg@4: 0.52414\tvalid's ndcg@5: 0.495782\n",
      "[21]\ttrain's ndcg@1: 0.899471\ttrain's ndcg@2: 0.865091\ttrain's ndcg@3: 0.823452\ttrain's ndcg@4: 0.798292\ttrain's ndcg@5: 0.775711\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.551502\tvalid's ndcg@3: 0.533831\tvalid's ndcg@4: 0.526809\tvalid's ndcg@5: 0.502265\n",
      "[22]\ttrain's ndcg@1: 0.910053\ttrain's ndcg@2: 0.873626\ttrain's ndcg@3: 0.834048\ttrain's ndcg@4: 0.807312\ttrain's ndcg@5: 0.781603\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.559148\tvalid's ndcg@3: 0.52106\tvalid's ndcg@4: 0.502842\tvalid's ndcg@5: 0.498304\n",
      "[23]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.878779\ttrain's ndcg@3: 0.830177\ttrain's ndcg@4: 0.808576\ttrain's ndcg@5: 0.78342\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.569923\tvalid's ndcg@3: 0.525583\tvalid's ndcg@4: 0.517279\tvalid's ndcg@5: 0.502516\n",
      "[24]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.878779\ttrain's ndcg@3: 0.836384\ttrain's ndcg@4: 0.80829\ttrain's ndcg@5: 0.781249\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.553007\tvalid's ndcg@3: 0.512636\tvalid's ndcg@4: 0.514515\tvalid's ndcg@5: 0.491784\n",
      "[25]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.878779\ttrain's ndcg@3: 0.836384\ttrain's ndcg@4: 0.810959\ttrain's ndcg@5: 0.780799\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.553007\tvalid's ndcg@3: 0.512636\tvalid's ndcg@4: 0.506509\tvalid's ndcg@5: 0.480663\n",
      "[26]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.878779\ttrain's ndcg@3: 0.832199\ttrain's ndcg@4: 0.806879\ttrain's ndcg@5: 0.778469\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.553007\tvalid's ndcg@3: 0.531258\tvalid's ndcg@4: 0.495313\tvalid's ndcg@5: 0.475124\n",
      "[27]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.878779\ttrain's ndcg@3: 0.832199\ttrain's ndcg@4: 0.81055\ttrain's ndcg@5: 0.782967\tvalid's ndcg@1: 0.571429\tvalid's ndcg@2: 0.559148\tvalid's ndcg@3: 0.535958\tvalid's ndcg@4: 0.501891\tvalid's ndcg@5: 0.480839\n",
      "[28]\ttrain's ndcg@1: 0.910053\ttrain's ndcg@2: 0.87229\ttrain's ndcg@3: 0.833441\ttrain's ndcg@4: 0.807912\ttrain's ndcg@5: 0.786958\tvalid's ndcg@1: 0.666667\tvalid's ndcg@2: 0.599121\tvalid's ndcg@3: 0.562828\tvalid's ndcg@4: 0.524243\tvalid's ndcg@5: 0.496094\n",
      "[29]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.880826\ttrain's ndcg@3: 0.836249\ttrain's ndcg@4: 0.812139\ttrain's ndcg@5: 0.791365\tvalid's ndcg@1: 0.666667\tvalid's ndcg@2: 0.599121\tvalid's ndcg@3: 0.562828\tvalid's ndcg@4: 0.545593\tvalid's ndcg@5: 0.510477\n",
      "[30]\ttrain's ndcg@1: 0.920635\ttrain's ndcg@2: 0.880826\ttrain's ndcg@3: 0.844939\ttrain's ndcg@4: 0.819369\ttrain's ndcg@5: 0.798339\tvalid's ndcg@1: 0.619048\tvalid's ndcg@2: 0.588345\tvalid's ndcg@3: 0.55458\tvalid's ndcg@4: 0.544069\tvalid's ndcg@5: 0.511236\n",
      "[31]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.88322\ttrain's ndcg@3: 0.844289\ttrain's ndcg@4: 0.822274\ttrain's ndcg@5: 0.802332\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.60781\tvalid's ndcg@3: 0.558305\tvalid's ndcg@4: 0.531156\tvalid's ndcg@5: 0.504182\n",
      "[32]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.885615\ttrain's ndcg@3: 0.848605\ttrain's ndcg@4: 0.820639\ttrain's ndcg@5: 0.80222\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.620091\tvalid's ndcg@3: 0.549082\tvalid's ndcg@4: 0.536827\tvalid's ndcg@5: 0.504944\n",
      "[33]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.891755\ttrain's ndcg@3: 0.848339\ttrain's ndcg@4: 0.822974\ttrain's ndcg@5: 0.800238\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.637007\tvalid's ndcg@3: 0.558305\tvalid's ndcg@4: 0.557843\tvalid's ndcg@5: 0.525284\n",
      "[34]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.895849\ttrain's ndcg@3: 0.845264\ttrain's ndcg@4: 0.823085\ttrain's ndcg@5: 0.800335\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.637007\tvalid's ndcg@3: 0.580651\tvalid's ndcg@4: 0.56042\tvalid's ndcg@5: 0.531689\n",
      "[35]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.895849\ttrain's ndcg@3: 0.846506\ttrain's ndcg@4: 0.825007\ttrain's ndcg@5: 0.800911\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.655429\tvalid's ndcg@3: 0.568679\tvalid's ndcg@4: 0.555798\tvalid's ndcg@5: 0.525591\n",
      "[36]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.895849\ttrain's ndcg@3: 0.847747\ttrain's ndcg@4: 0.82604\ttrain's ndcg@5: 0.803396\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.630867\tvalid's ndcg@3: 0.568503\tvalid's ndcg@4: 0.53697\tvalid's ndcg@5: 0.505068\n",
      "[37]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.844673\ttrain's ndcg@4: 0.826151\ttrain's ndcg@5: 0.806964\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.641643\tvalid's ndcg@3: 0.57675\tvalid's ndcg@4: 0.543831\tvalid's ndcg@5: 0.517277\n",
      "[38]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.85088\ttrain's ndcg@4: 0.826867\ttrain's ndcg@5: 0.806692\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.647783\tvalid's ndcg@3: 0.577725\tvalid's ndcg@4: 0.557986\tvalid's ndcg@5: 0.512913\n",
      "[39]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.853363\ttrain's ndcg@4: 0.827154\ttrain's ndcg@5: 0.810117\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.641643\tvalid's ndcg@3: 0.57675\tvalid's ndcg@4: 0.559843\tvalid's ndcg@5: 0.51661\n",
      "[40]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.854605\ttrain's ndcg@4: 0.829039\ttrain's ndcg@5: 0.814081\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.653924\tvalid's ndcg@3: 0.578701\tvalid's ndcg@4: 0.561466\tvalid's ndcg@5: 0.518019\n",
      "[41]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.855846\ttrain's ndcg@4: 0.828442\ttrain's ndcg@5: 0.813306\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.641643\tvalid's ndcg@3: 0.580475\tvalid's ndcg@4: 0.560273\tvalid's ndcg@5: 0.521148\n",
      "[42]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.855846\ttrain's ndcg@4: 0.831964\ttrain's ndcg@5: 0.816608\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.641643\tvalid's ndcg@3: 0.580475\tvalid's ndcg@4: 0.560273\tvalid's ndcg@5: 0.527396\n",
      "[43]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.899595\ttrain's ndcg@3: 0.860176\ttrain's ndcg@4: 0.833458\ttrain's ndcg@5: 0.814881\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.653924\tvalid's ndcg@3: 0.593598\tvalid's ndcg@4: 0.565853\tvalid's ndcg@5: 0.536409\n",
      "[44]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.906083\ttrain's ndcg@3: 0.862659\ttrain's ndcg@4: 0.835523\ttrain's ndcg@5: 0.819452\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.66157\tvalid's ndcg@3: 0.588277\tvalid's ndcg@4: 0.558757\tvalid's ndcg@5: 0.520031\n",
      "[45]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.906083\ttrain's ndcg@3: 0.862659\ttrain's ndcg@4: 0.834634\ttrain's ndcg@5: 0.823111\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.607698\tvalid's ndcg@4: 0.566907\tvalid's ndcg@5: 0.537524\n",
      "[46]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.906083\ttrain's ndcg@3: 0.862659\ttrain's ndcg@4: 0.837303\ttrain's ndcg@5: 0.822476\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.66157\tvalid's ndcg@3: 0.59945\tvalid's ndcg@4: 0.560046\tvalid's ndcg@5: 0.531564\n",
      "[47]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.906083\ttrain's ndcg@3: 0.862659\ttrain's ndcg@4: 0.846807\ttrain's ndcg@5: 0.827489\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.66157\tvalid's ndcg@3: 0.59945\tvalid's ndcg@4: 0.560046\tvalid's ndcg@5: 0.525116\n",
      "[48]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.910177\ttrain's ndcg@3: 0.863309\ttrain's ndcg@4: 0.853463\ttrain's ndcg@5: 0.829895\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.607698\tvalid's ndcg@4: 0.566907\tvalid's ndcg@5: 0.533159\n",
      "[49]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.910177\ttrain's ndcg@3: 0.865792\ttrain's ndcg@4: 0.853749\ttrain's ndcg@5: 0.830144\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.660064\tvalid's ndcg@3: 0.605747\tvalid's ndcg@4: 0.565284\tvalid's ndcg@5: 0.53175\n",
      "[50]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.910177\ttrain's ndcg@3: 0.869517\ttrain's ndcg@4: 0.854179\ttrain's ndcg@5: 0.838077\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.660064\tvalid's ndcg@3: 0.605747\tvalid's ndcg@4: 0.57329\tvalid's ndcg@5: 0.538705\n",
      "[51]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.910177\ttrain's ndcg@3: 0.871438\ttrain's ndcg@4: 0.85442\ttrain's ndcg@5: 0.83731\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.660064\tvalid's ndcg@3: 0.605747\tvalid's ndcg@4: 0.57329\tvalid's ndcg@5: 0.538705\n",
      "[52]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.882805\ttrain's ndcg@4: 0.855579\ttrain's ndcg@5: 0.835566\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.666205\tvalid's ndcg@3: 0.610447\tvalid's ndcg@4: 0.574531\tvalid's ndcg@5: 0.543949\n",
      "[53]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.85971\ttrain's ndcg@5: 0.841281\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.666205\tvalid's ndcg@3: 0.610447\tvalid's ndcg@4: 0.563856\tvalid's ndcg@5: 0.54923\n",
      "[54]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.865937\ttrain's ndcg@5: 0.845303\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.666205\tvalid's ndcg@3: 0.62162\tvalid's ndcg@4: 0.57582\tvalid's ndcg@5: 0.551316\n",
      "[55]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.864158\ttrain's ndcg@5: 0.845839\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.649289\tvalid's ndcg@3: 0.612397\tvalid's ndcg@4: 0.568148\tvalid's ndcg@5: 0.548948\n",
      "[56]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.866939\ttrain's ndcg@5: 0.848176\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.649289\tvalid's ndcg@3: 0.612397\tvalid's ndcg@4: 0.565479\tvalid's ndcg@5: 0.544324\n",
      "[57]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.869607\ttrain's ndcg@5: 0.852724\tvalid's ndcg@1: 0.698413\tvalid's ndcg@2: 0.649289\tvalid's ndcg@3: 0.612397\tvalid's ndcg@4: 0.568148\tvalid's ndcg@5: 0.546865\n",
      "[58]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.916317\ttrain's ndcg@3: 0.887771\ttrain's ndcg@4: 0.869607\ttrain's ndcg@5: 0.853418\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.660064\tvalid's ndcg@3: 0.620645\tvalid's ndcg@4: 0.575009\tvalid's ndcg@5: 0.552826\n",
      "[59]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.888421\ttrain's ndcg@4: 0.868369\ttrain's ndcg@5: 0.853776\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.622595\tvalid's ndcg@4: 0.576631\tvalid's ndcg@5: 0.558401\n",
      "[60]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.888421\ttrain's ndcg@4: 0.870148\ttrain's ndcg@5: 0.857699\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.630044\tvalid's ndcg@4: 0.57749\tvalid's ndcg@5: 0.563313\n",
      "[61]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.888421\ttrain's ndcg@4: 0.871647\ttrain's ndcg@5: 0.859228\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.630044\tvalid's ndcg@4: 0.57749\tvalid's ndcg@5: 0.563313\n",
      "[62]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.888421\ttrain's ndcg@4: 0.872821\ttrain's ndcg@5: 0.862383\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.622595\tvalid's ndcg@4: 0.576631\tvalid's ndcg@5: 0.547788\n",
      "[63]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.889893\ttrain's ndcg@4: 0.87301\ttrain's ndcg@5: 0.86256\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.622595\tvalid's ndcg@4: 0.576631\tvalid's ndcg@5: 0.547788\n",
      "[64]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.889893\ttrain's ndcg@4: 0.87301\ttrain's ndcg@5: 0.863948\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.622595\tvalid's ndcg@4: 0.573962\tvalid's ndcg@5: 0.54753\n",
      "[65]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.889893\ttrain's ndcg@4: 0.87301\ttrain's ndcg@5: 0.86547\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.672345\tvalid's ndcg@3: 0.622595\tvalid's ndcg@4: 0.573962\tvalid's ndcg@5: 0.54753\n",
      "[66]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.895539\ttrain's ndcg@4: 0.871901\ttrain's ndcg@5: 0.86676\tvalid's ndcg@1: 0.68254\tvalid's ndcg@2: 0.657978\tvalid's ndcg@3: 0.619048\tvalid's ndcg@4: 0.565674\tvalid's ndcg@5: 0.540328\n",
      "[67]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.893618\ttrain's ndcg@4: 0.875218\ttrain's ndcg@5: 0.869496\tvalid's ndcg@1: 0.68254\tvalid's ndcg@2: 0.657978\tvalid's ndcg@3: 0.630221\tvalid's ndcg@4: 0.566962\tvalid's ndcg@5: 0.547696\n",
      "[68]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.895539\ttrain's ndcg@4: 0.875459\ttrain's ndcg@5: 0.869719\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.647202\tvalid's ndcg@3: 0.621973\tvalid's ndcg@4: 0.560102\tvalid's ndcg@5: 0.541735\n",
      "[69]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.895539\ttrain's ndcg@4: 0.875459\ttrain's ndcg@5: 0.871107\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.647202\tvalid's ndcg@3: 0.6108\tvalid's ndcg@4: 0.558813\tvalid's ndcg@5: 0.540616\n",
      "[70]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.895539\ttrain's ndcg@4: 0.875459\ttrain's ndcg@5: 0.871107\tvalid's ndcg@1: 0.68254\tvalid's ndcg@2: 0.651837\tvalid's ndcg@3: 0.618072\tvalid's ndcg@4: 0.564862\tvalid's ndcg@5: 0.539624\n",
      "[71]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.895539\ttrain's ndcg@4: 0.876349\ttrain's ndcg@5: 0.871193\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.644653\tvalid's ndcg@3: 0.601401\tvalid's ndcg@4: 0.559\tvalid's ndcg@5: 0.540778\n",
      "[72]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.898022\ttrain's ndcg@4: 0.876635\ttrain's ndcg@5: 0.871442\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.644653\tvalid's ndcg@3: 0.601401\tvalid's ndcg@4: 0.559\tvalid's ndcg@5: 0.53453\n",
      "[73]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.920411\ttrain's ndcg@3: 0.898022\ttrain's ndcg@4: 0.880803\ttrain's ndcg@5: 0.873207\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.644653\tvalid's ndcg@3: 0.590227\tvalid's ndcg@4: 0.547036\tvalid's ndcg@5: 0.530384\n",
      "[74]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.901155\ttrain's ndcg@4: 0.885188\ttrain's ndcg@5: 0.872852\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.644653\tvalid's ndcg@3: 0.601401\tvalid's ndcg@4: 0.548325\tvalid's ndcg@5: 0.525256\n",
      "[75]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.903638\ttrain's ndcg@4: 0.887254\ttrain's ndcg@5: 0.873258\tvalid's ndcg@1: 0.68254\tvalid's ndcg@2: 0.651837\tvalid's ndcg@3: 0.606899\tvalid's ndcg@4: 0.566242\tvalid's ndcg@5: 0.532515\n",
      "[76]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.903638\ttrain's ndcg@4: 0.889033\ttrain's ndcg@5: 0.872042\tvalid's ndcg@1: 0.603175\tvalid's ndcg@2: 0.633877\tvalid's ndcg@3: 0.593153\tvalid's ndcg@4: 0.554808\tvalid's ndcg@5: 0.52258\n",
      "[77]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.904879\ttrain's ndcg@4: 0.891845\ttrain's ndcg@5: 0.874551\tvalid's ndcg@1: 0.603175\tvalid's ndcg@2: 0.633877\tvalid's ndcg@3: 0.593153\tvalid's ndcg@4: 0.554808\tvalid's ndcg@5: 0.528828\n",
      "[78]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.908604\ttrain's ndcg@4: 0.892274\ttrain's ndcg@5: 0.876313\tvalid's ndcg@1: 0.603175\tvalid's ndcg@2: 0.633877\tvalid's ndcg@3: 0.593153\tvalid's ndcg@4: 0.554808\tvalid's ndcg@5: 0.528828\n",
      "[79]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.908604\ttrain's ndcg@4: 0.894053\ttrain's ndcg@5: 0.87647\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.62264\tvalid's ndcg@3: 0.584552\tvalid's ndcg@4: 0.555659\tvalid's ndcg@5: 0.529568\n",
      "[80]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.924505\ttrain's ndcg@3: 0.912328\ttrain's ndcg@4: 0.894483\ttrain's ndcg@5: 0.880887\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.62264\tvalid's ndcg@3: 0.584552\tvalid's ndcg@4: 0.555659\tvalid's ndcg@5: 0.529568\n",
      "[81]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.926552\ttrain's ndcg@3: 0.912653\ttrain's ndcg@4: 0.897422\ttrain's ndcg@5: 0.882232\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.641061\tvalid's ndcg@3: 0.598652\tvalid's ndcg@4: 0.559382\tvalid's ndcg@5: 0.532802\n",
      "[82]\ttrain's ndcg@1: 0.941799\ttrain's ndcg@2: 0.926552\ttrain's ndcg@3: 0.912653\ttrain's ndcg@4: 0.897422\ttrain's ndcg@5: 0.884315\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.641061\tvalid's ndcg@3: 0.594927\tvalid's ndcg@4: 0.558952\tvalid's ndcg@5: 0.532429\n",
      "[83]\ttrain's ndcg@1: 0.952381\ttrain's ndcg@2: 0.926899\ttrain's ndcg@3: 0.914161\ttrain's ndcg@4: 0.898676\ttrain's ndcg@5: 0.886393\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.641061\tvalid's ndcg@3: 0.594927\tvalid's ndcg@4: 0.558952\tvalid's ndcg@5: 0.532429\n",
      "[84]\ttrain's ndcg@1: 0.952381\ttrain's ndcg@2: 0.935087\ttrain's ndcg@3: 0.917944\ttrain's ndcg@4: 0.900044\ttrain's ndcg@5: 0.887696\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.641061\tvalid's ndcg@3: 0.594927\tvalid's ndcg@4: 0.558952\tvalid's ndcg@5: 0.532429\n",
      "[85]\ttrain's ndcg@1: 0.952381\ttrain's ndcg@2: 0.939181\ttrain's ndcg@3: 0.92244\ttrain's ndcg@4: 0.902362\ttrain's ndcg@5: 0.891227\tvalid's ndcg@1: 0.634921\tvalid's ndcg@2: 0.62264\tvalid's ndcg@3: 0.592001\tvalid's ndcg@4: 0.556518\tvalid's ndcg@5: 0.524066\n",
      "[86]\ttrain's ndcg@1: 0.952381\ttrain's ndcg@2: 0.939181\ttrain's ndcg@3: 0.92244\ttrain's ndcg@4: 0.903251\ttrain's ndcg@5: 0.893389\tvalid's ndcg@1: 0.68254\tvalid's ndcg@2: 0.633416\tvalid's ndcg@3: 0.589075\tvalid's ndcg@4: 0.554084\tvalid's ndcg@5: 0.5282\n",
      "[87]\ttrain's ndcg@1: 0.952381\ttrain's ndcg@2: 0.939181\ttrain's ndcg@3: 0.92244\ttrain's ndcg@4: 0.903251\ttrain's ndcg@5: 0.89626\tvalid's ndcg@1: 0.650794\tvalid's ndcg@2: 0.60781\tvalid's ndcg@3: 0.580651\tvalid's ndcg@4: 0.533733\tvalid's ndcg@5: 0.518826\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttrain's ndcg@1: 0.931217\ttrain's ndcg@2: 0.893454\ttrain's ndcg@3: 0.844673\ttrain's ndcg@4: 0.826151\ttrain's ndcg@5: 0.806964\tvalid's ndcg@1: 0.746032\tvalid's ndcg@2: 0.641643\tvalid's ndcg@3: 0.57675\tvalid's ndcg@4: 0.543831\tvalid's ndcg@5: 0.517277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Convert query IDs to group lengths (LightGBM requires group info for ranking tasks)\n",
    "def get_group_sizes(qids):\n",
    "    \"\"\"\n",
    "    Compute group sizes for LightGBM from query IDs.\n",
    "\n",
    "    Args:\n",
    "        qids (list): List of query IDs.\n",
    "\n",
    "    Returns:\n",
    "        group_sizes (list): Number of samples per query group.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    qid_counts = Counter(qids)\n",
    "    return [qid_counts[qid] for qid in sorted(qid_counts.keys())]\n",
    "\n",
    "# Group sizes\n",
    "group_train = get_group_sizes(qids_train)\n",
    "group_valid = get_group_sizes(qids_valid)\n",
    "\n",
    "# Prepare LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=train_data)\n",
    "\n",
    "# LightGBM parameters for LambdaMART\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'max_position': 10,  # Evaluate NDCG@10\n",
    "    'label_gain': [0, 1, 3, 7],  # Relevance levels (adjust based on dataset)\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'num_iterations': 100,  # Number of boosting iterations\n",
    "}\n",
    "\n",
    "# Train the LambdaMART model\n",
    "eval_result = {}\n",
    "print(\"Training LightGBM LambdaMART model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(),\n",
    "        lgb.record_evaluation(eval_result)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H E A V Y gbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tree 1\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7691412518043562\n",
      "========================================\n",
      "Training Tree 2\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.776891532705439\n",
      "========================================\n",
      "Training Tree 3\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7853137349546829\n",
      "========================================\n",
      "Training Tree 4\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7811929831633312\n",
      "========================================\n",
      "Training Tree 5\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7898327315656435\n",
      "========================================\n",
      "Training Tree 6\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7966073036955552\n",
      "========================================\n",
      "Training Tree 7\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7962148133401229\n",
      "========================================\n",
      "Training Tree 8\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7989808756882015\n",
      "========================================\n",
      "Training Tree 9\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7994412597551389\n",
      "========================================\n",
      "Training Tree 10\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8016322821929259\n",
      "========================================\n",
      "Training Tree 11\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.801504103172621\n",
      "========================================\n",
      "Training Tree 12\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8054161156003489\n",
      "========================================\n",
      "Training Tree 13\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.809879609417339\n",
      "========================================\n",
      "Training Tree 14\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8049041232573456\n",
      "========================================\n",
      "Training Tree 15\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8081767222585063\n",
      "========================================\n",
      "Training Tree 16\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.7997761131158441\n",
      "========================================\n",
      "Training Tree 17\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8088583072283327\n",
      "========================================\n",
      "Training Tree 18\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8117118308288648\n",
      "========================================\n",
      "Training Tree 19\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8140248361914254\n",
      "========================================\n",
      "Training Tree 20\n",
      "Precomputing Lambdas\n",
      "Training Regressor\n",
      "Evaluating NDCG\n",
      "NDCG: 0.8149567399137702\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from LambdaMARTy import LambdaMART\n",
    "from collections import Counter\n",
    "\n",
    "sorted_indices = np.argsort(qids_train)[::-1]\n",
    "q_train2 = [Counter(qids_train)[i+1] for i in range(len(set(qids_train)))]\n",
    "x_train2 = X_train[sorted_indices]\n",
    "y_train2 = y_train[sorted_indices].reshape(-1,1)\n",
    "\n",
    "marty = LambdaMART(\n",
    "    min_samples_per_node=5,\n",
    "    max_depth=20,\n",
    "    impurity_measure='variance',\n",
    "    learning_rate=0.1,\n",
    "    num_trees=20\n",
    ")\n",
    "\n",
    "marty.build_forest(q_train2, x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wawa = np.array([1,2,3]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wawa.reshape(-1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
